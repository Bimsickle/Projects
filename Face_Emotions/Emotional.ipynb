{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Emotional Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialise the training and validation generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'data/train'\n",
    "val_dir = 'data/test'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                train_dir, \n",
    "                target_size = (48,48), \n",
    "                batch_size =64, \n",
    "                color_mode = \"grayscale\", \n",
    "                class_mode = 'categorical')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "                val_dir, \n",
    "                target_size = (48,48), \n",
    "                batch_size =64, \n",
    "                color_mode = \"grayscale\", \n",
    "                class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile & Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-39-99d54789b14f>:3: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/50\n",
      "448/448 [==============================] - 199s 443ms/step - loss: 1.8046 - accuracy: 0.2570 - val_loss: 1.7089 - val_accuracy: 0.3449\n",
      "Epoch 2/50\n",
      "448/448 [==============================] - 149s 333ms/step - loss: 1.6238 - accuracy: 0.3678 - val_loss: 1.5298 - val_accuracy: 0.4191\n",
      "Epoch 3/50\n",
      "448/448 [==============================] - 150s 335ms/step - loss: 1.5148 - accuracy: 0.4190 - val_loss: 1.4447 - val_accuracy: 0.4473\n",
      "Epoch 4/50\n",
      "448/448 [==============================] - 153s 341ms/step - loss: 1.4365 - accuracy: 0.4507 - val_loss: 1.3860 - val_accuracy: 0.4693\n",
      "Epoch 5/50\n",
      "448/448 [==============================] - 151s 338ms/step - loss: 1.3721 - accuracy: 0.4767 - val_loss: 1.3215 - val_accuracy: 0.4962\n",
      "Epoch 6/50\n",
      "448/448 [==============================] - 152s 339ms/step - loss: 1.3218 - accuracy: 0.4984 - val_loss: 1.2862 - val_accuracy: 0.5103\n",
      "Epoch 7/50\n",
      "448/448 [==============================] - 151s 338ms/step - loss: 1.2748 - accuracy: 0.5162 - val_loss: 1.2506 - val_accuracy: 0.5262\n",
      "Epoch 8/50\n",
      "448/448 [==============================] - 152s 340ms/step - loss: 1.2367 - accuracy: 0.5349 - val_loss: 1.2215 - val_accuracy: 0.5329\n",
      "Epoch 9/50\n",
      "448/448 [==============================] - 150s 336ms/step - loss: 1.2065 - accuracy: 0.5464 - val_loss: 1.2018 - val_accuracy: 0.5480\n",
      "Epoch 10/50\n",
      "448/448 [==============================] - 151s 336ms/step - loss: 1.1775 - accuracy: 0.5564 - val_loss: 1.1708 - val_accuracy: 0.5552\n",
      "Epoch 11/50\n",
      "448/448 [==============================] - 148s 331ms/step - loss: 1.1486 - accuracy: 0.5697 - val_loss: 1.1718 - val_accuracy: 0.5499\n",
      "Epoch 12/50\n",
      "448/448 [==============================] - 147s 329ms/step - loss: 1.1242 - accuracy: 0.5770 - val_loss: 1.1697 - val_accuracy: 0.5530\n",
      "Epoch 13/50\n",
      "448/448 [==============================] - 148s 330ms/step - loss: 1.0989 - accuracy: 0.5889 - val_loss: 1.1348 - val_accuracy: 0.5741\n",
      "Epoch 14/50\n",
      "448/448 [==============================] - 155s 346ms/step - loss: 1.0764 - accuracy: 0.5985 - val_loss: 1.1236 - val_accuracy: 0.5760\n",
      "Epoch 15/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 1.0521 - accuracy: 0.6066 - val_loss: 1.1166 - val_accuracy: 0.5763\n",
      "Epoch 16/50\n",
      "448/448 [==============================] - 147s 329ms/step - loss: 1.0338 - accuracy: 0.6130 - val_loss: 1.1066 - val_accuracy: 0.5804\n",
      "Epoch 17/50\n",
      "448/448 [==============================] - 147s 327ms/step - loss: 1.0074 - accuracy: 0.6277 - val_loss: 1.0954 - val_accuracy: 0.5868\n",
      "Epoch 18/50\n",
      "448/448 [==============================] - 147s 329ms/step - loss: 0.9875 - accuracy: 0.6331 - val_loss: 1.0891 - val_accuracy: 0.5910\n",
      "Epoch 19/50\n",
      "448/448 [==============================] - 146s 327ms/step - loss: 0.9664 - accuracy: 0.6432 - val_loss: 1.0843 - val_accuracy: 0.5924\n",
      "Epoch 20/50\n",
      "448/448 [==============================] - 147s 329ms/step - loss: 0.9454 - accuracy: 0.6489 - val_loss: 1.0804 - val_accuracy: 0.5956\n",
      "Epoch 21/50\n",
      "448/448 [==============================] - 146s 327ms/step - loss: 0.9171 - accuracy: 0.6600 - val_loss: 1.0790 - val_accuracy: 0.5942\n",
      "Epoch 22/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 0.9003 - accuracy: 0.6670 - val_loss: 1.0663 - val_accuracy: 0.6017\n",
      "Epoch 23/50\n",
      "448/448 [==============================] - 146s 326ms/step - loss: 0.8717 - accuracy: 0.6782 - val_loss: 1.0680 - val_accuracy: 0.6016\n",
      "Epoch 24/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 0.8513 - accuracy: 0.6868 - val_loss: 1.0672 - val_accuracy: 0.6035\n",
      "Epoch 25/50\n",
      "448/448 [==============================] - 146s 327ms/step - loss: 0.8313 - accuracy: 0.6947 - val_loss: 1.0753 - val_accuracy: 0.6060\n",
      "Epoch 26/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 0.8095 - accuracy: 0.7018 - val_loss: 1.0639 - val_accuracy: 0.6095\n",
      "Epoch 27/50\n",
      "448/448 [==============================] - 149s 334ms/step - loss: 0.7907 - accuracy: 0.7097 - val_loss: 1.0534 - val_accuracy: 0.6102\n",
      "Epoch 28/50\n",
      "448/448 [==============================] - 149s 333ms/step - loss: 0.7671 - accuracy: 0.7187 - val_loss: 1.0749 - val_accuracy: 0.6097\n",
      "Epoch 29/50\n",
      "448/448 [==============================] - 148s 330ms/step - loss: 0.7446 - accuracy: 0.7276 - val_loss: 1.0687 - val_accuracy: 0.6143\n",
      "Epoch 30/50\n",
      "448/448 [==============================] - 147s 327ms/step - loss: 0.7215 - accuracy: 0.7349 - val_loss: 1.0591 - val_accuracy: 0.6179\n",
      "Epoch 31/50\n",
      "448/448 [==============================] - 148s 330ms/step - loss: 0.7026 - accuracy: 0.7453 - val_loss: 1.0886 - val_accuracy: 0.6154\n",
      "Epoch 32/50\n",
      "448/448 [==============================] - 148s 330ms/step - loss: 0.6796 - accuracy: 0.7513 - val_loss: 1.0696 - val_accuracy: 0.6166\n",
      "Epoch 33/50\n",
      "448/448 [==============================] - 148s 331ms/step - loss: 0.6673 - accuracy: 0.7562 - val_loss: 1.0678 - val_accuracy: 0.6203\n",
      "Epoch 34/50\n",
      "448/448 [==============================] - 148s 330ms/step - loss: 0.6378 - accuracy: 0.7698 - val_loss: 1.0656 - val_accuracy: 0.6244\n",
      "Epoch 35/50\n",
      "448/448 [==============================] - 148s 330ms/step - loss: 0.6187 - accuracy: 0.7771 - val_loss: 1.0768 - val_accuracy: 0.6272\n",
      "Epoch 36/50\n",
      "448/448 [==============================] - 147s 329ms/step - loss: 0.6042 - accuracy: 0.7801 - val_loss: 1.0947 - val_accuracy: 0.6200\n",
      "Epoch 37/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 0.5824 - accuracy: 0.7877 - val_loss: 1.0913 - val_accuracy: 0.6244\n",
      "Epoch 38/50\n",
      "448/448 [==============================] - 146s 326ms/step - loss: 0.5608 - accuracy: 0.7975 - val_loss: 1.0975 - val_accuracy: 0.6211\n",
      "Epoch 39/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 0.5448 - accuracy: 0.8044 - val_loss: 1.1162 - val_accuracy: 0.6242\n",
      "Epoch 40/50\n",
      "448/448 [==============================] - 147s 329ms/step - loss: 0.5271 - accuracy: 0.8099 - val_loss: 1.1109 - val_accuracy: 0.6217\n",
      "Epoch 41/50\n",
      "448/448 [==============================] - 147s 327ms/step - loss: 0.5134 - accuracy: 0.8143 - val_loss: 1.1092 - val_accuracy: 0.6226\n",
      "Epoch 42/50\n",
      "448/448 [==============================] - 147s 328ms/step - loss: 0.4981 - accuracy: 0.8215 - val_loss: 1.1297 - val_accuracy: 0.6257\n",
      "Epoch 43/50\n",
      "448/448 [==============================] - 146s 326ms/step - loss: 0.4811 - accuracy: 0.8264 - val_loss: 1.1251 - val_accuracy: 0.6251\n",
      "Epoch 44/50\n",
      "448/448 [==============================] - 147s 327ms/step - loss: 0.4635 - accuracy: 0.8307 - val_loss: 1.1267 - val_accuracy: 0.6242\n",
      "Epoch 45/50\n",
      "448/448 [==============================] - 145s 324ms/step - loss: 0.4444 - accuracy: 0.8378 - val_loss: 1.1513 - val_accuracy: 0.6281\n",
      "Epoch 46/50\n",
      "448/448 [==============================] - 145s 324ms/step - loss: 0.4346 - accuracy: 0.8409 - val_loss: 1.1736 - val_accuracy: 0.6272\n",
      "Epoch 47/50\n",
      "448/448 [==============================] - 144s 322ms/step - loss: 0.4167 - accuracy: 0.8523 - val_loss: 1.1692 - val_accuracy: 0.6274\n",
      "Epoch 48/50\n",
      "448/448 [==============================] - 145s 324ms/step - loss: 0.4085 - accuracy: 0.8540 - val_loss: 1.1680 - val_accuracy: 0.6257\n",
      "Epoch 49/50\n",
      "448/448 [==============================] - 145s 323ms/step - loss: 0.3992 - accuracy: 0.8545 - val_loss: 1.1893 - val_accuracy: 0.6296\n",
      "Epoch 50/50\n",
      "448/448 [==============================] - 146s 325ms/step - loss: 0.3860 - accuracy: 0.8619 - val_loss: 1.2004 - val_accuracy: 0.6275\n"
     ]
    }
   ],
   "source": [
    "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
    "\n",
    "emotion_model_info = emotion_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=28709 // 64,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=7178 // 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect bounding boxes around faces "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    bounding_box = cv2.CascadeClassifier('data/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor = 1.3, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (255,0,0),2)\n",
    "        roi_gray_frame = gray_frame[y:y +h, x:x +w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48,48)), -1),0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        cv2.imshow('Video', cv2.resize(frame, (1200,860), interpolation = cv2.INTER_CUBIC))\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GUI Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "emotion_model = Sequential()\n",
    "\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "emotion_model.load_weights('model.h5')\n",
    "\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "emotion_dict = {0: \"   Angry   \", 1: \"Disgusted\", 2: \"  Fearful  \", 3: \"   Happy   \", 4: \"  Neutral  \", 5: \"    Sad    \", 6: \"Surprised\"}\n",
    "\n",
    "\n",
    "emoji_dist={0:\"./emojis/angry.png\",2:\"./emojis/disgusted.png\",2:\"./emojis/fearful.png\",3:\"./emojis/happy.png\",4:\"./emojis/neutral.png\",5:\"./emojis/sad.png\",6:\"./emojis/surpriced.png\"}\n",
    "\n",
    "global last_frame1                                    \n",
    "last_frame1 = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "global cap1\n",
    "show_text=[0]\n",
    "def show_vid():      \n",
    "    cap1 = cv2.VideoCapture(0)                                 \n",
    "    if not cap1.isOpened():                             \n",
    "        print(\"cant open the camera1\")\n",
    "    flag1, frame1 = cap1.read()\n",
    "    frame1 = cv2.resize(frame1,(600,500))\n",
    "\n",
    "    bounding_box = cv2.CascadeClassifier('data/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame,scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame1, (x, y-50), (x+w, y+h+10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        prediction = emotion_model.predict(cropped_img)\n",
    "        \n",
    "        maxindex = int(np.argmax(prediction))\n",
    "        cv2.putText(frame1, emotion_dict[maxindex], (x+20, y-60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        show_text[0]=maxindex\n",
    "    if flag1 is None:\n",
    "        print (\"Major error!\")\n",
    "    elif flag1:\n",
    "        global last_frame1\n",
    "        last_frame1 = frame1.copy()\n",
    "        pic = cv2.cvtColor(last_frame1, cv2.COLOR_BGR2RGB)     \n",
    "        img = Image.fromarray(pic)\n",
    "        imgtk = ImageTk.PhotoImage(image=img)\n",
    "        lmain.imgtk = imgtk\n",
    "        lmain.configure(image=imgtk)\n",
    "        lmain.after(10, show_vid)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        exit()\n",
    "\n",
    "\n",
    "def show_vid2():\n",
    "    frame2=cv2.imread(emoji_dist[show_text[0]])\n",
    "    pic2=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "    img2=Image.fromarray(frame2)\n",
    "    imgtk2=ImageTk.PhotoImage(image=img2)\n",
    "    lmain2.imgtk2=imgtk2\n",
    "    lmain3.configure(text=emotion_dict[show_text[0]],font=('arial',45,'bold'))\n",
    "    \n",
    "    lmain2.configure(image=imgtk2)\n",
    "    lmain2.after(10, show_vid2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root=tk.Tk()   \n",
    "#     img = ImageTk.PhotoImage(Image.open(\"logo.png\"))\n",
    "#     heading = Label(root,image=img,bg='black')\n",
    "    \n",
    "#     heading.pack() \n",
    "#     heading2=Label(root,text=\"Photo to Emoji\",pady=20, font=('arial',45,'bold'),bg='black',fg='#CDCDCD')                                 \n",
    "    \n",
    "#     heading2.pack()\n",
    "    lmain = tk.Label(master=root,padx=50,bd=10)\n",
    "    lmain2 = tk.Label(master=root,bd=10)\n",
    "\n",
    "    lmain3=tk.Label(master=root,bd=10,fg=\"#CDCDCD\",bg='black')\n",
    "    lmain.pack(side=LEFT)\n",
    "    lmain.place(x=50,y=250)\n",
    "    lmain3.pack()\n",
    "    lmain3.place(x=960,y=250)\n",
    "    lmain2.pack(side=RIGHT)\n",
    "    lmain2.place(x=900,y=350)\n",
    "    \n",
    "\n",
    "\n",
    "    root.title(\"Photo To Emoji\")            \n",
    "    root.geometry(\"1400x900+100+10\") \n",
    "    root['bg']='black'\n",
    "    exitbutton = Button(root, text='Quit',fg=\"red\",command=root.destroy,font=('arial',25,'bold')).pack(side = BOTTOM)\n",
    "    show_vid()\n",
    "    show_vid2()\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
